{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the MNIST dataset.\n",
    "\n",
    "Some of the benchmark results on MNIST include can be found [on Yann LeCun's page](https://webcache.googleusercontent.com/search?q=cache:stAVPik6onEJ:yann.lecun.com/exdb/mnist) and include:\n",
    "\n",
    "88% [Lecun et al., 1998](https://hal.science/hal-03926082/document)\n",
    "\n",
    "95.3% [Lecun et al., 1998](https://hal.science/hal-03926082v1/document)\n",
    "\n",
    "99.65% [Ciresan et al., 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf)\n",
    "\n",
    "\n",
    "MNIST is a great dataset for sanity checking your models, since the accuracy levels achieved by large convolutional neural networks and small linear models are both quite high. This makes it important to be familiar with the data.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets==8.1.5\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.12\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 40.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==8.1.5->-r requirements.txt (line 1)) (7.13.0)\n",
      "Collecting jupyterlab-widgets~=3.0.12\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 77.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==8.1.5->-r requirements.txt (line 1)) (4.3.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (45.2.0.post20200209)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (2.5.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (4.4.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets==8.1.5->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.1.5->-r requirements.txt (line 1)) (0.1.8)\n",
      "Installing collected packages: widgetsnbextension, comm, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed comm-0.1.4 ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "# Restart the Kernel after you execute this command.\n",
    "\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: Restart the Kernel at this moment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list if you intend to .\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "MNIST is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `MNIST` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/vision/stable/datasets.html#mnist)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6965b6e0884bb4ab405ec24f1a8970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec84f002ec3a4e9db4c019702b0b2026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424fe168868b429d9e6a0817921947d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dac7fae1da49268a92652db323ca32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "# Training transforms with augmentations\n",
    "train_transform = transforms.Compose([\n",
    "                                        transforms.RandomCrop(28, padding=4),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.RandomRotation(degrees=10),\n",
    "                                        transforms.ColorJitter(brightness=0.45, contrast=0.45, saturation=0.45),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Test transforms \n",
    "test_transform = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST training and testing datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "\n",
    "# Create training set and define training dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# Create test set and define test dataloader (no shuffle for test data)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your preprocessing\n",
    "\n",
    "In your own words, why did you choose the transforms you chose? If you didn't use any preprocessing steps, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Justification\n",
    "\n",
    "## Augmentation techniques to improve generalization:\n",
    "- **RandomCrop**: Simulates slight shifts for better alignment.\n",
    "- **RandomHorizontalFlip**: Adds variability.\n",
    "- **RandomRotation(10°)**: Accounts for handwriting slants.\n",
    "- **ColorJitter**: Simulates brightness and contrast variations.\n",
    "- **ToTensor**: Converts images to tensors.\n",
    "- **Normalize**: Standardizes pixel values for stable learning.\n",
    "\n",
    "## Why No Augmentations for Testing?\n",
    "Only `ToTensor` and `Normalize` are applied to preserve the original test set for fair evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader – DO NOT CHANGE THE CONTENTS! ##\n",
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(int(labels[i].detach()))\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore data\n",
    "show5(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset.\n",
    "Use any architecture you like. \n",
    "\n",
    "*Note*: If you did not flatten your tensors in your transforms or as part of your preprocessing and you are using only `Linear` layers, make sure to use the `Flatten` layer in your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MnistNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1 * 28 * 28, 256)  # Input: 784 pixels (flattened image), Output: 256 neurons\n",
    "        self.fc2 = nn.Linear(256, 128)  \n",
    "        self.fc3 = nn.Linear(128, 64)  \n",
    "        self.fc4 = nn.Linear(64, 10)            # Output layer: 10 neurons for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))  \n",
    "        x = F.log_softmax(self.fc4(x), dim=1)  # Output layer with log-softmax\n",
    "\n",
    "        return x\n",
    "\n",
    "model = MnistNetwork()\n",
    "model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the loss function (Cross-Entropy Loss / multi-class classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss **during** each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [250/938], Loss: 1.339, Accuracy: 53.344%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6638090b41ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Get inputs and labels (move to device if using GPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1178\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontrast_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msaturation_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \"\"\"\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0menhancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageEnhance.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImageStat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegenerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m             \u001b[0;31m# determine default mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs     = 10\n",
    "train_loss     = [] \n",
    "train_accuracy = []  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct      = 0\n",
    "    total        = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)   # Get inputs and labels (move to device if using GPU)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss    = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        _, predicted  = torch.max(outputs.data, dim = 1)\n",
    "        total        += labels.size(0)\n",
    "        correct      += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print loss and accuracy every 250 batches\n",
    "        if i % 250 == 249:  \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/250:.3f}, Accuracy: {100 * correct / total:.3f}%\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Calculate and store average loss and accuracy per epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accuracy.append(100 * correct / total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)  # Left subplot for loss\n",
    "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)  # Right subplot for accuracy\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, label='Training Accuracy', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 90%, great work, but see if you can push a bit further! \n",
    "If your accuracy is under 90%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Set model to evaluation mode (optional)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get the predicted class with the highest probability\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.3f}%\")\n",
    "\n",
    "# Test the model\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Once your model is done training, try tweaking your hyperparameters and training again below to improve your accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model \n",
    "model_filename = \"JoeEl_MNIST_model.pth\"\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "print(f\"Model saved to: {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
